<!DOCTYPE html>
<html>
<head>
    <title>Real-Time Object Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            width: 100%;
            height: 100%;
            overflow: hidden;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        #video, #canvas {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
    </style>
</head>
<body>
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>

    <script>
        // Initialization
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const context = canvas.getContext('2d');

        // Load ONNX model
        async function loadModel() {
            const model = await ort.InferenceSession.create('best.onnx');
            return model;
        }

        // Start video stream
        navigator.mediaDevices.getUserMedia({ video: true })
            .then(stream => {
                video.srcObject = stream;
                video.onloadedmetadata = () => {
                    video.play();
                };
            })
            .catch(err => {
                console.error('Error accessing the camera', err);
            });

        // Main detection function
        async function detectObjects() {
            const model = await loadModel();
            video.addEventListener('play', async () => {
                while (true) {
                    if (video.paused || video.ended) {
                        break;
                    }
                    await processFrame(model);
                    await new Promise(r => setTimeout(r, 100)); // Adjust detection frequency
                }
            });
        }

        // Process each frame
        async function processFrame(model) {
            // Preprocess input (resize to 1x3x640x640, normalize, etc.)
            const preprocessedData = preprocess(video);

            // Run model
            const tensor = new ort.Tensor('float32', preprocessedData.data, [1, 3, 640, 640]);
            const result = await model.run({ 'images': tensor });
            console.log(result); // Add this line to inspect the output structure


            // Postprocess and draw detections
            drawDetections(result, video.videoWidth, video.videoHeight);
        }

        // Preprocessing function (resize, normalize)
        function preprocess(videoElement) {
        const width = 640;
        const height = 640;
        const canvas = document.createElement('canvas');
        canvas.width = width;
        canvas.height = height;
        const ctx = canvas.getContext('2d');

        // Draw the video frame to the canvas
        ctx.drawImage(videoElement, 0, 0, width, height);

        // Extract pixel data
        const imageData = ctx.getImageData(0, 0, width, height);
        const data = imageData.data;

        // Normalize pixel data
        const input = new Float32Array(3 * width * height);
        for (let i = 0; i < height * width; ++i) {
            input[i * 3] = (data[i * 4] / 255) - 0.5;       // Red
            input[i * 3 + 1] = (data[i * 4 + 1] / 255) - 0.5; // Green
            input[i * 3 + 2] = (data[i * 4 + 2] / 255) - 0.5; // Blue
        }

        return { data: input, width, height };
    }

        // Function to draw detections on canvas
        function drawDetections(output, originalWidth, originalHeight) {
        const ctx = canvas.getContext('2d');
        ctx.clearRect(0, 0, canvas.width, canvas.height);

    // Assuming the output tensor's data is a Float32Array with the shape [1, 5, 8400]
    // Convert tensor to array for easier handling
    const detectionData = output.output0.data;
    const numDetections = output.output0.dims[2]; // should be 8400
    const dataPerDetection = output.output0.dims[1]; // should be 5

    for (let i = 0; i < numDetections; i++) {
        const offset = i * dataPerDetection;
        const x_center = detectionData[offset] * originalWidth;
        const y_center = detectionData[offset + 1] * originalHeight;
        const width = detectionData[offset + 2] * originalWidth;
        const height = detectionData[offset + 3] * originalHeight;
        const confidence = detectionData[offset + 4];

        // Check for a confidence threshold to determine if the detection is valid
        if (confidence > 0.9) {
            // Convert center coordinates to top-left coordinates
            const x = x_center - (width / 2);
            const y = y_center - (height / 2);

            // Draw bounding box and label
            ctx.strokeStyle = '#FF0000'; // red color for bounding box
            ctx.lineWidth = 3;
            ctx.strokeRect(x, y, width, height);

            // Label for 'tablo' class
            const label = `Tablo, Conf: ${(confidence * 100).toFixed(2)}%`;
            ctx.fillStyle = '#FF0000';
            ctx.fillText(label, x, y);
        }
    }
}


        // Start detection
        detectObjects();
    </script>
</body>
</html>
